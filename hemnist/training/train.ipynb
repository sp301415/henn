{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.420179\n",
      "Epoch: 2 \tTraining Loss: 0.143187\n",
      "Epoch: 3 \tTraining Loss: 0.095321\n",
      "Epoch: 4 \tTraining Loss: 0.073644\n",
      "Epoch: 5 \tTraining Loss: 0.062036\n",
      "Epoch: 6 \tTraining Loss: 0.050439\n",
      "Epoch: 7 \tTraining Loss: 0.045990\n",
      "Epoch: 8 \tTraining Loss: 0.039238\n",
      "Epoch: 9 \tTraining Loss: 0.036038\n",
      "Epoch: 10 \tTraining Loss: 0.033039\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(73)\n",
    "\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, hidden=64, output=10):\n",
    "        super(ConvNet, self).__init__()        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=7, padding=0, stride=3)\n",
    "        self.fc1 = torch.nn.Linear(256, hidden)\n",
    "        self.fc2 = torch.nn.Linear(hidden, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        # the model uses the square activation function\n",
    "        x = x * x\n",
    "        # flattening while keeping the batch axis\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc1(x)\n",
    "        x = x * x\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, n_epochs=10):\n",
    "    # model in training mode\n",
    "    model.train()\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "    \n",
    "    # model in evaluation mode\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model = train(model, train_loader, criterion, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.112927\n",
      "\n",
      "Test Accuracy of 0: 99% (972/980)\n",
      "Test Accuracy of 1: 99% (1128/1135)\n",
      "Test Accuracy of 2: 98% (1019/1032)\n",
      "Test Accuracy of 3: 98% (999/1010)\n",
      "Test Accuracy of 4: 97% (960/982)\n",
      "Test Accuracy of 5: 96% (858/892)\n",
      "Test Accuracy of 6: 97% (933/958)\n",
      "Test Accuracy of 7: 95% (985/1028)\n",
      "Test Accuracy of 8: 97% (945/974)\n",
      "Test Accuracy of 9: 98% (991/1009)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9790/10000)\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    # initialize lists to monitor test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    # model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        # convert output probabilities to predicted class\n",
    "        \n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # calculate and print avg test loss\n",
    "    test_loss = test_loss/len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
    "\n",
    "    for label in range(10):\n",
    "        print(\n",
    "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
    "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "    )\n",
    "    \n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv1\n",
    "for i, f in enumerate(model.conv1.weight.detach().numpy()):\n",
    "    np.savetxt(f\"../model/CW{i}.csv\", f[0], delimiter=\",\")\n",
    "np.savetxt(f\"../model/CB.csv\", model.conv1.bias.detach().numpy(), delimiter=\",\")\n",
    "\n",
    "# LC1\n",
    "np.savetxt(f\"../model/LW1.csv\", model.fc1.weight.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt(f\"../model/LB1.csv\", model.fc1.bias.detach().numpy(), delimiter=\",\")\n",
    "\n",
    "# LC2\n",
    "np.savetxt(f\"../model/LW2.csv\", model.fc2.weight.detach().numpy(), delimiter=\",\")\n",
    "np.savetxt(f\"../model/LB2.csv\", model.fc2.bias.detach().numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/xy1pd25s1j3b39t4lkp1cc4r0000gn/T/ipykernel_3744/63792817.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  img = torch.tensor([img])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.9283e-01,  3.9283e-01,  3.9283e-01,  3.9283e-01,  3.9283e-01,\n",
       "           3.9283e-01,  3.9283e-01,  3.9283e-01],\n",
       "         [ 2.3382e-01, -2.7359e-01,  4.7459e-01,  3.7585e-01,  2.9909e-01,\n",
       "           4.2801e-01,  8.8767e-01,  3.0873e-01],\n",
       "         [ 2.6895e-02,  2.6429e-01,  8.3365e-02,  1.9869e-01,  1.0552e-01,\n",
       "           4.2180e-03,  1.5783e+00,  2.2101e-01],\n",
       "         [ 3.4234e-01,  3.2934e-01,  1.8478e-01,  2.3201e-01, -9.3780e-01,\n",
       "           1.2003e+00,  1.7609e-01,  3.2475e-01],\n",
       "         [ 3.9283e-01,  3.9283e-01,  3.9283e-01,  1.5059e-01, -1.1325e+00,\n",
       "           2.2095e+00, -3.0141e-01,  3.9283e-01],\n",
       "         [ 3.9283e-01,  3.9283e-01,  3.6778e-01, -9.5790e-01,  1.8679e+00,\n",
       "           1.8811e-01,  3.2118e-01,  3.9283e-01],\n",
       "         [ 3.9283e-01,  3.9283e-01, -2.6800e-01, -2.4297e-01,  1.6880e+00,\n",
       "          -1.2719e-01,  3.9283e-01,  3.9283e-01],\n",
       "         [ 3.9283e-01,  3.9283e-01, -1.1077e+00,  2.1386e+00, -1.4975e-02,\n",
       "           3.6795e-01,  3.9283e-01,  3.9283e-01]],\n",
       "\n",
       "        [[-1.4558e-01, -1.4558e-01, -1.4558e-01, -1.4558e-01, -1.4558e-01,\n",
       "          -1.4558e-01, -1.4558e-01, -1.4558e-01],\n",
       "         [-2.7844e-01, -6.0630e-01, -1.7411e-01,  6.1662e-01,  8.6408e-01,\n",
       "           9.3519e-01,  6.7944e-01,  9.5752e-02],\n",
       "         [-8.8871e-02, -3.7807e-01, -1.4967e+00, -1.8787e+00, -1.9386e+00,\n",
       "          -1.8288e+00, -8.9636e-01, -3.4600e-02],\n",
       "         [-1.3139e-01,  1.3388e-01,  3.3526e-01,  4.7782e-01,  1.4744e-01,\n",
       "          -3.1031e-01, -6.3329e-01, -2.1455e-01],\n",
       "         [-1.4558e-01, -1.4558e-01, -1.4558e-01, -2.4879e-01, -3.2854e-01,\n",
       "          -4.7696e-02, -5.0586e-01, -1.4558e-01],\n",
       "         [-1.4558e-01, -1.4558e-01, -1.6504e-01, -3.1914e-01, -3.4847e-01,\n",
       "          -4.9424e-01, -2.1687e-01, -1.4558e-01],\n",
       "         [-1.4558e-01, -1.4558e-01, -3.8280e-01, -3.2894e-01,  1.1101e-01,\n",
       "          -4.7306e-01, -1.4558e-01, -1.4558e-01],\n",
       "         [-1.4558e-01, -1.4558e-01, -8.6548e-01, -1.1271e+00, -7.8435e-01,\n",
       "          -1.6682e-01, -1.4558e-01, -1.4558e-01]],\n",
       "\n",
       "        [[-1.4478e-01, -1.4478e-01, -1.4478e-01, -1.4478e-01, -1.4478e-01,\n",
       "          -1.4478e-01, -1.4478e-01, -1.4478e-01],\n",
       "         [-3.7983e-01, -1.6007e+00, -1.6977e+00, -1.2088e+00, -1.1958e+00,\n",
       "          -1.0096e+00, -2.4171e-01, -2.2965e-01],\n",
       "         [ 1.6214e-01,  2.3097e-01,  1.0239e-01, -3.4113e-03, -1.7988e-01,\n",
       "          -1.1098e+00, -4.0216e-01, -3.0473e-02],\n",
       "         [-1.2108e-01, -1.2622e-01,  1.2072e-02, -6.7247e-02, -5.1717e-01,\n",
       "          -1.1447e+00, -3.0661e-01, -5.4756e-02],\n",
       "         [-1.4478e-01, -1.4478e-01, -1.4478e-01, -3.7733e-01, -3.4313e-01,\n",
       "          -1.0099e+00,  1.8215e-01, -1.4478e-01],\n",
       "         [-1.4478e-01, -1.4478e-01, -1.6140e-01, -7.5073e-01, -1.0190e+00,\n",
       "          -3.3637e-01, -3.7051e-02, -1.4478e-01],\n",
       "         [-1.4478e-01, -1.4478e-01, -6.8558e-01, -8.2319e-01, -8.0355e-01,\n",
       "           1.4488e-01, -1.4478e-01, -1.4478e-01],\n",
       "         [-1.4478e-01, -1.4478e-01, -2.8620e-01, -1.8095e+00, -9.2196e-02,\n",
       "          -1.0131e-01, -1.4478e-01, -1.4478e-01]],\n",
       "\n",
       "        [[-1.8148e-04, -1.8148e-04, -1.8148e-04, -1.8148e-04, -1.8148e-04,\n",
       "          -1.8148e-04, -1.8148e-04, -1.8148e-04],\n",
       "         [-2.4161e-01,  1.6052e-01,  5.3353e-01, -6.6549e-02, -6.1730e-02,\n",
       "           9.2225e-02,  3.3579e-01,  6.2962e-02],\n",
       "         [ 1.0808e-01,  7.9813e-01, -4.0761e-02,  3.0150e-01,  1.5567e-01,\n",
       "           3.3526e-01,  2.4745e-02, -2.5411e-01],\n",
       "         [ 7.9907e-03,  8.1910e-02,  1.1501e-01,  2.2733e-02, -5.7445e-01,\n",
       "           1.8234e+00, -1.2682e+00,  5.3872e-02],\n",
       "         [-1.8148e-04, -1.8148e-04, -1.8148e-04, -2.3202e-01,  1.2233e+00,\n",
       "           5.4458e-01, -6.8662e-01, -1.8148e-04],\n",
       "         [-1.8148e-04, -1.8148e-04, -1.7866e-02, -3.4724e-01,  2.0555e+00,\n",
       "          -1.3579e+00,  4.3778e-02, -1.8148e-04],\n",
       "         [-1.8148e-04, -1.8148e-04, -5.8981e-01,  1.9603e+00, -1.7721e-01,\n",
       "          -3.6812e-01, -1.8148e-04, -1.8148e-04],\n",
       "         [-1.8148e-04, -1.8148e-04,  5.6493e-01,  1.9314e+00, -1.2510e+00,\n",
       "           8.3933e-03, -1.8148e-04, -1.8148e-04]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv \n",
    "\n",
    "with open(\"../mnist_test.csv\") as f:\n",
    "    line = f.readline().strip().split(\",\")\n",
    "    label = line[0]\n",
    "    img = np.array([float(x) / 255 for x in line[1:]], dtype=np.float32).reshape((28, 28))\n",
    "\n",
    "img = torch.tensor([img])\n",
    "model.conv1(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training-3nn9iWAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5eb3c237dab97d7ed10e11cc174e4574990200a6682a49da7123dcb1d191d72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
